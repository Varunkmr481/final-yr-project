{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "04721366-1d21-4d7e-9152-74deb982ef17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#opencv\n",
    "import cv2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1974c539-ad0b-466f-bb74-88605a141cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import insightface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b132cd-4c88-46a9-8dcd-0ce1127b17e4",
   "metadata": {},
   "source": [
    "### 1. Configure insight Face model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9ccafaee-e275-4ccb-a98f-63d0a0517285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from insightface.app import FaceAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6e3629-2b3e-4370-b36b-1d75f94ec374",
   "metadata": {},
   "source": [
    "#### a. Buffalo l model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5ee4adab-15a3-401a-bd9c-31516625963d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: insightface_model\\models\\buffalo_l\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: insightface_model\\models\\buffalo_l\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: insightface_model\\models\\buffalo_l\\det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: insightface_model\\models\\buffalo_l\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: insightface_model\\models\\buffalo_l\\w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "app_l = FaceAnalysis(name='buffalo_l', root='insightface_model', providers=['CPUExecutionProvider'])\n",
    "app_l.prepare(ctx_id=0, det_size=(640,640))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db4810-c4c9-45d5-80ff-226a71e5cfbc",
   "metadata": {},
   "source": [
    "#### b. Buffalo sc model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4043ac5f-8dad-4d94-9886-953447ed9aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: insightface_model\\models\\buffalo_sc\\det_500m.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: insightface_model\\models\\buffalo_sc\\w600k_mbf.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "app_sc = FaceAnalysis(name='buffalo_sc', root='insightface_model', providers=['CPUExecutionProvider'])\n",
    "app_sc.prepare(ctx_id=0, det_size=(640,640))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0bb3b-b202-4d7e-b214-362eb28dfca3",
   "metadata": {},
   "source": [
    "#### c. Buffalo m model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "42529fab-2236-4e83-b4f6-9ba596018cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app_sc = FaceAnalysis(name='buffalo_m', root='insightface_model', providers=['CPUExecutionProvider'])\n",
    "# app_sc.prepare(ctx_id=0, det_size=(640,640))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f0868b-686a-4f24-9270-ed76d664ec75",
   "metadata": {},
   "source": [
    "#### d. antelopev2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5e9bdc07-50eb-45c8-bb73-791842c57b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: insightface_model\\models\\antelopev2\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: insightface_model\\models\\antelopev2\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: insightface_model\\models\\antelopev2\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: insightface_model\\models\\antelopev2\\glintr100.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: insightface_model\\models\\antelopev2\\scrfd_10g_bnkps.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "# app_sc = FaceAnalysis(name='antelopev2', root='insightface_model', providers=['CPUExecutionProvider'])\n",
    "# app_sc.prepare(ctx_id=0, det_size=(640,640))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501a0783-25e1-43d2-aca7-17ba5609f3d5",
   "metadata": {},
   "source": [
    "### e. Buffalo s model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bfd0aedd-dd29-4456-9fcf-6e9a15872c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: insightface_model\\models\\buffalo_s\\1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: insightface_model\\models\\buffalo_s\\2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: insightface_model\\models\\buffalo_s\\det_500m.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: insightface_model\\models\\buffalo_s\\genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
      "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
      "find model: insightface_model\\models\\buffalo_s\\w600k_mbf.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
      "set det-size: (640, 640)\n"
     ]
    }
   ],
   "source": [
    "# app_sc = FaceAnalysis(name='buffalo_s', root='insightface_model', providers=['CPUExecutionProvider'])\n",
    "# app_sc.prepare(ctx_id=0, det_size=(640,640))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d423d49-0dce-41c9-805d-8c57e2221adb",
   "metadata": {},
   "source": [
    "### 2. Load the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9df78d-1e4c-4654-a5f7-7bb766dcb320",
   "metadata": {},
   "source": [
    "#### a. buffalo_l model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fbc20512-35e7-4707-8f70-65ee0b68842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"test_images/test_13.jpeg\")\n",
    "\n",
    "# Resize image to fit the screen\n",
    "screen_res = 1280, 1280  # Adjust this based on your screen resolution\n",
    "scale_width = screen_res[0] / img.shape[1]\n",
    "scale_height = screen_res[1] / img.shape[0]\n",
    "scale = min(scale_width, scale_height)\n",
    "window_width = int(img.shape[1] * scale)\n",
    "window_height = int(img.shape[0] * scale)\n",
    "\n",
    "# Resize the image\n",
    "resized_img = cv2.resize(img, (window_width, window_height))\n",
    "\n",
    "cv2.imshow('image',resized_img)\n",
    "# as soon as esc key or button x is pressed, then close the window\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad40d321-89fe-4525-a012-84d7841f2052",
   "metadata": {},
   "source": [
    "#### b. buffalo_sc model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "df6ae6bd-9ffc-4636-a563-0f0df2363a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv2.imread('test_images/test_13.jpeg')\n",
    "\n",
    "# Resize image to fit the screen\n",
    "screen_res = 1800,1050  # Adjust this based on your screen resolution\n",
    "scale_width = screen_res[0] / img1.shape[1]\n",
    "scale_height = screen_res[1] / img1.shape[0]\n",
    "scale = min(scale_width, scale_height)\n",
    "window_width = int(img1.shape[1] * scale)\n",
    "window_height = int(img1.shape[0] * scale)\n",
    "\n",
    "# Resize the image\n",
    "resized_img1 = cv2.resize(img1, (window_width, window_height))\n",
    "\n",
    "cv2.imshow(\"image1\",resized_img1)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c72f8e0-b4b4-4a32-b59f-0c1793b1b355",
   "metadata": {},
   "source": [
    "#### c. buffalo_m model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "697cc783-d22a-4654-b990-fc2c06303f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img2 = cv2.imread('test-image_3.jpg')\n",
    "\n",
    "# # Resize image to fit the screen\n",
    "# screen_res = 1800,1050  # Adjust this based on your screen resolution\n",
    "# scale_width = screen_res[0] / img1.shape[1]\n",
    "# scale_height = screen_res[1] / img1.shape[0]\n",
    "# scale = min(scale_width, scale_height)\n",
    "# window_width = int(img1.shape[1] * scale)\n",
    "# window_height = int(img1.shape[0] * scale)\n",
    "\n",
    "# # Resize the image\n",
    "# resized_img2 = cv2.resize(img2, (window_width, window_height))\n",
    "\n",
    "# cv2.imshow(\"image2\",resized_img2)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc2ba07-173b-48d4-86b8-b60b8021a97d",
   "metadata": {},
   "source": [
    "### d. antelopev2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "13fb72f1-b3e4-43d4-8071-e34cf89cbc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img3 = cv2.imread('test-image_3.jpg')\n",
    "\n",
    "# # Resize image to fit the screen\n",
    "# screen_res = 1800,1050  # Adjust this based on your screen resolution\n",
    "# scale_width = screen_res[0] / img1.shape[1]\n",
    "# scale_height = screen_res[1] / img1.shape[0]\n",
    "# scale = min(scale_width, scale_height)\n",
    "# window_width = int(img1.shape[1] * scale)\n",
    "# window_height = int(img1.shape[0] * scale)\n",
    "\n",
    "# # Resize the image\n",
    "# resized_img3 = cv2.resize(img3, (window_width, window_height))\n",
    "\n",
    "# cv2.imshow(\"image3\",resized_img3)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18140ed5-7a7b-4ab4-903e-2828c6fcee18",
   "metadata": {},
   "source": [
    "## e. buffalo_s model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a0bd5bc-1834-4e5a-ba5d-3f51d8a2d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img4 = cv2.imread('test-image_3.jpg')\n",
    "\n",
    "# # Resize image to fit the screen\n",
    "# screen_res = 1800,1050  # Adjust this based on your screen resolution\n",
    "# scale_width = screen_res[0] / img4.shape[1]\n",
    "# scale_height = screen_res[1] / img4.shape[0]\n",
    "# scale = min(scale_width, scale_height)\n",
    "# window_width = int(img1.shape[1] * scale)\n",
    "# window_height = int(img1.shape[0] * scale)\n",
    "\n",
    "# # Resize the image\n",
    "# resized_img4 = cv2.resize(img4, (window_width, window_height))\n",
    "\n",
    "# cv2.imshow(\"image4\",resized_img4)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e767d224-6f29-4632-a8e1-19d738438b49",
   "metadata": {},
   "source": [
    "### 3. Apply image to insightface model and get results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb91c7-2143-4425-a868-8422eded1b13",
   "metadata": {},
   "source": [
    "### a. Buffalo_l model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "79df9805-40c1-4a41-a66f-f5e6da8b692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applied the image to the model buffalo_l\n",
    "results_l = app_l.get(resized_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "927d8641-e0dd-48f0-ad35-8a0b2d6dfd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "64b2f024-870d-48b5-8dbf-551372c4d35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 3)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image has 21 images and the model detected 21 images \n",
    "type(results_l), len(results_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "874110d0-2878-4fa5-b994-14c36664ca85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bbox', 'kps', 'det_score', 'landmark_3d_68', 'pose', 'landmark_2d_106', 'gender', 'age', 'embedding'])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will work will 3 : bbox : bounding box, det_score : face detection score, embedding : facial features\n",
    "results_l[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "89ba0964-2562-434a-9b98-1df13b6f6d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([589.09753, 138.47356, 766.6547 , 379.3796 ], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gives the bbox info about the image\n",
    "results_l[0]['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "77fb55f1-af7b-4800-8e46-ce6528e56aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key pts : [[641.1909  233.59782]\n",
      " [718.60187 239.7619 ]\n",
      " [678.4769  284.37122]\n",
      " [639.80133 311.0371 ]\n",
      " [711.52466 316.5517 ]]\n",
      "detection score : 0.89180684\n",
      "gender : 1\n",
      "age : 35\n"
     ]
    }
   ],
   "source": [
    "# gives the key points info about the image\n",
    "print('key pts :', results_l[0]['kps'])\n",
    "\n",
    "# gives the detection score info about the image\n",
    "print('detection score :',results_l[0]['det_score'])\n",
    "\n",
    "# gives the info about the gender and age \n",
    "print(\"gender :\",results_l[0]['gender']) # 1 is for male, 0 is for female\n",
    "print(\"age :\",results_l[0]['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "6be1fd84-5f1a-4d96-aa8c-f6bcda3a7bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw the bounding box of each detected face\n",
    "resized_img_copy = resized_img.copy();\n",
    "gender_encode = ['female', 'male']\n",
    "\n",
    "for res in results_l:\n",
    "    x1, y1, x2, y2 = res['bbox'].astype(int)\n",
    "\n",
    "    # draw rectange\n",
    "    cv2.rectangle(resized_img_copy, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "\n",
    "    # draw keypoints\n",
    "    kps = res['kps'].astype(int)\n",
    "    for k1, k2 in kps:\n",
    "        cv2.circle(resized_img_copy, (k1,k2), 2, (0,255,255), -1)\n",
    "\n",
    "    # detection score\n",
    "    score = \"score: {}%\".format(int(res['det_score']*100))\n",
    "    cv2.putText(resized_img_copy, score, (x1,y1), cv2.FONT_HERSHEY_DUPLEX, 0.5, (255,0,255))\n",
    "\n",
    "    # gender and age\n",
    "    gender = gender_encode[res['gender']]\n",
    "    # age = res['age']\n",
    "    # age_gender = f\"{gender}::{age}\"\n",
    "    cv2.putText(resized_img_copy, gender, (x1,y2+10), cv2.FONT_HERSHEY_DUPLEX, 0.5, (255,0,255))\n",
    "\n",
    "cv2.imshow('bbox', resized_img_copy)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c9a02a22-cec0-4ba5-a996-1e80f6d7c9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[641.1909 , 233.59782],\n",
       "       [718.60187, 239.7619 ],\n",
       "       [678.4769 , 284.37122],\n",
       "       [639.80133, 311.0371 ],\n",
       "       [711.52466, 316.5517 ]], dtype=float32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gives the key points info about the image\n",
    "results_l[0]['kps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4225e39c-eb06-4900-9e9e-3c3b1235b0a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.89180684)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gives the detection score info about the image\n",
    "results_l[0]['det_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "44f2f294-9a4d-46ab-af43-fcb557324776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "# gives the info about the gender and age \n",
    "print(results_l[0]['gender']) # 1 is for male, 0 is for female\n",
    "print(results_l[0]['age'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d1ea1-3691-4ebe-bf38-06fd6870a2ef",
   "metadata": {},
   "source": [
    "### b. Buffalo_sc model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ceaa59d0-16c7-46fe-b0f1-3bfe2e6fdf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applied the image to the model buffalo_l\n",
    "results_sc = app_sc.get(resized_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6ac79c02-2b44-4a34-a0d0-e9571d84170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# print(results_sc)\n",
    "print(len(results_sc))\n",
    "# print(results_sc[0]['kps'])\n",
    "# print(results_sc[90]['det_score'])\n",
    "# results_sc[0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7017042b-f0ca-4c4c-92af-46449a56632a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bbox', 'kps', 'det_score', 'landmark_3d_68', 'pose', 'landmark_2d_106', 'gender', 'age', 'embedding'])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_sc[0].keys()\n",
    "# print(results_sc[0]['bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d00aaea9-66a8-45fd-914c-dbc61c52a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_img1_copy = resized_img1.copy();\n",
    "gender_encode2 = ['female', 'male']\n",
    "\n",
    "for res in results_sc:\n",
    "    x1, y1, x2, y2 = res['bbox'].astype(int)\n",
    "\n",
    "    # draw bounding box\n",
    "    cv2.rectangle(resized_img1_copy, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "\n",
    "    # draw keypts\n",
    "    kps = res['kps'].astype(int)\n",
    "    for k1, k2 in kps:\n",
    "        cv2.circle(resized_img1_copy, (k1,k2), 2, (0,255,255), -1)\n",
    "\n",
    "    # add detection score\n",
    "    score = \"{}%\".format(int(res['det_score']*100))\n",
    "    cv2.putText(resized_img1_copy, score, (x1,y1), cv2.FONT_HERSHEY_DUPLEX, 0.5, (255,0,0))\n",
    "\n",
    "    \n",
    "cv2.imshow('bbox', resized_img1_copy)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d80bec-625e-4af4-9418-81fec2a483c4",
   "metadata": {},
   "source": [
    "### c. Buffalo_m model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bdb31367-7e93-434b-b964-384c918772f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applied the image to the model buffalo_m\n",
    "# results_m = app_sc.get(resized_img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "69987f50-f471-44b3-a379-608cf32dc327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# print(results_sc)\n",
    "# print(len(results_sc))\n",
    "# print(results_sc[0]['kps'])\n",
    "# print(results_sc[90]['det_score'])\n",
    "# results_sc[0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6e510c1c-7a00-4652-98d4-8b50cc63a548",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bbox', 'kps', 'det_score', 'landmark_3d_68', 'pose', 'landmark_2d_106', 'gender', 'age', 'embedding'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results_sc[0].keys()\n",
    "# print(results_sc[0]['bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "da17fbb8-eba4-4837-87d0-fd780b64c800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resized_img2_copy = resized_img2.copy();\n",
    "# gender_encode2 = ['female', 'male']\n",
    "\n",
    "# for res in results_sc:\n",
    "#     x1, y1, x2, y2 = res['bbox'].astype(int)\n",
    "\n",
    "#     # draw bounding box\n",
    "#     cv2.rectangle(resized_img2_copy, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "\n",
    "#     # draw keypts\n",
    "#     kps = res['kps'].astype(int)\n",
    "#     for k1, k2 in kps:\n",
    "#         cv2.circle(resized_img2_copy, (k1,k2), 2, (0,255,255), -1)\n",
    "\n",
    "#     # add detection score\n",
    "#     score = \"{}%\".format(int(res['det_score']*100))\n",
    "#     cv2.putText(resized_img2_copy, score, (x1,y1), cv2.FONT_HERSHEY_DUPLEX, 0.5, (255,0,0))\n",
    "\n",
    "    \n",
    "# cv2.imshow('bbox', resized_img2_copy)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364e6fa5-3d96-4570-aa14-fbc2522f69cb",
   "metadata": {},
   "source": [
    "### d. antelope model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "55028520-2486-4b57-9238-b79cae2570fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applied the image to the model buffalo_m\n",
    "# results_an = app_sc.get(resized_img3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "44a97a90-c1e7-4e6d-9193-c0f2ede81541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# print(results_sc)\n",
    "# print(len(results_an))\n",
    "# print(results_sc[0]['kps'])\n",
    "# print(results_sc[90]['det_score'])\n",
    "# results_sc[0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2990da94-1dc4-4d04-a8b8-72dda453fd15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bbox', 'kps', 'det_score', 'landmark_3d_68', 'pose', 'landmark_2d_106', 'gender', 'age', 'embedding'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results_sc[0].keys()\n",
    "# print(results_sc[0]['bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e63fa7c4-291b-438e-a35e-4d42a95a96b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resized_img3_copy = resized_img3.copy();\n",
    "# gender_encode2 = ['female', 'male']\n",
    "\n",
    "# for res in results_sc:\n",
    "#     x1, y1, x2, y2 = res['bbox'].astype(int)\n",
    "\n",
    "#     # draw bounding box\n",
    "#     cv2.rectangle(resized_img3_copy, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "\n",
    "#     # draw keypts\n",
    "#     kps = res['kps'].astype(int)\n",
    "#     for k1, k2 in kps:\n",
    "#         cv2.circle(resized_img3_copy, (k1,k2), 2, (0,255,255), -1)\n",
    "\n",
    "#     # add detection score\n",
    "#     score = \"{}%\".format(int(res['det_score']*100))\n",
    "#     cv2.putText(resized_img3_copy, score, (x1,y1), cv2.FONT_HERSHEY_DUPLEX, 0.5, (255,0,0))\n",
    "\n",
    "    \n",
    "# cv2.imshow('bbox', resized_img3_copy)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4473557e-6edf-4daf-980d-cc5b33d6b271",
   "metadata": {},
   "source": [
    "### e. buffalo_s model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c32adbfd-6001-4dce-97a7-86bef5104a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applied the image to the model buffalo_m\n",
    "# results_s = app_sc.get(resized_img4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "be057e16-5c57-4ab6-8a66-051c113b2807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# print(results_sc)\n",
    "# print(len(results_s))\n",
    "# print(results_sc[0]['kps'])\n",
    "# print(results_sc[90]['det_score'])\n",
    "# results_sc[0]['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2fb57f71-d5bd-4302-85e7-9b212d31306e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bbox', 'kps', 'det_score', 'landmark_3d_68', 'pose', 'landmark_2d_106', 'gender', 'age', 'embedding'])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results_s[0].keys()\n",
    "\n",
    "# print(results_sc[0]['bbox'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dccf7148-48a9-4132-b679-e48cc81879d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resized_img4_copy = resized_img4.copy();\n",
    "# gender_encode2 = ['female', 'male']\n",
    "\n",
    "# for res in results_sc:\n",
    "#     x1, y1, x2, y2 = res['bbox'].astype(int)\n",
    "\n",
    "#     # draw bounding box\n",
    "#     cv2.rectangle(resized_img4_copy, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "\n",
    "#     # draw keypts\n",
    "#     kps = res['kps'].astype(int)\n",
    "#     for k1, k2 in kps:\n",
    "#         cv2.circle(resized_img4_copy, (k1,k2), 2, (0,255,255), -1)\n",
    "\n",
    "#     # add detection score\n",
    "#     score = \"{}%\".format(int(res['det_score']*100))\n",
    "#     cv2.putText(resized_img4_copy, score, (x1,y1), cv2.FONT_HERSHEY_DUPLEX, 0.5, (255,0,0))\n",
    "\n",
    "    \n",
    "# cv2.imshow('bbox', resized_img4_copy)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa44399a-4a86-4522-9c22-830c4201b91f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
